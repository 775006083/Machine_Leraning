{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXO 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons expérimenter quelques approches \"traditionnelles\" de la classification des textes. Ces approches sont antérieures à la révolution de l'apprentissage en profondeur dans le traitement du langage naturel, mais sont souvent des moyens rapides et efficaces de former un classificateur de texte.\n",
    "\n",
    "\n",
    "Pour nos données, nous allons travailler avec l'ensemble de données 20 Newsgroups, une collection classique de documents texte qui est souvent utilisée comme référence pour les modèles de classification de texte. L'ensemble contient des textes sur divers sujets, allant de l'informatique à la religion. Certains sujets sont étroitement liés les uns aux autres (tels que « Matériel IBM PC » et « Matériel Mac »), tandis que d'autres sont très différents (tels que « religion » ou « hockey »). Les 20 Newsgroups sont livrés avec la bibliothèque d'apprentissage automatique Scikit-learn, notre principal outil pour cet exercice. Il a été divisé en un ensemble de formation de 11 314 textes et un ensemble de test de 7 532 textes.\n",
    "\n",
    "\n",
    "Pour cette fois on vous laisse utiliser vos connaisances pour faire l'ensemble des traitements que vous jugerai nécessaire pour un bon classifieur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installation de librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (4.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (2.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Chargement des bases de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_ds_train = fetch_20newsgroups(subset = 'train', shuffle = True, random_state = 42)\n",
    "from pprint import pprint\n",
    "pprint(list(news_ds_train.target_names))\n",
    "#pprint(list(news_ds_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc de code suivant, plusieurs éléments sont affichés :\n",
    "* La liste des topics abordés dans le jeu de données\n",
    "* Le nombre de news \n",
    "* Le contenu de la 1ère news\n",
    "* Le topic de la 1ère news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des topics\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Nombre de news\n",
      "11314\n",
      "\n",
      "Affichage de la 1ère news\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Topic de la 1ère news\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print(\"Liste des topics\")\n",
    "print(news_ds_train.target_names)\n",
    "print(\"\\nNombre de news\")\n",
    "print(len(news_ds_train.data))\n",
    "print(\"\\nAffichage de la 1ère news\")\n",
    "print(news_ds_train.data[0])\n",
    "print(\"\\nTopic de la 1ère news\")\n",
    "print(news_ds_train.target_names[news_ds_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Classification de textes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Classification avec sklearn\n",
    "Dans cette section, nous allons entrainer un modèle pour classifier les news du jeu de données 20newsgroup avec la librairie sklearn.\n",
    "\n",
    "On commence par importer le jeu de données\n",
    "* alt.atheism\n",
    "* comp.graphics\n",
    "* rec.motorcycles\n",
    "* sci.crypt\n",
    "* sci.med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer à compter le nombre d'occurence des mots dans le corpus que nous venons de créer (ce corpus correspond à la variable *news_ds_train.data*).\n",
    "\n",
    "Puis nous allons ensuite Vectoriser  ce corpus avec TF-IDF.\n",
    "\n",
    "Ainsi nous enregistrerons  ces features du corpus dans la variable *corpus_X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.crypt', 'sci.med']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.crypt','sci.med']\n",
    "news_ds_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = 42)\n",
    "print(news_ds_train.target_names)\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.crypt','sci.med']\n",
    "news_ds_test = fetch_20newsgroups(subset = 'test', categories = categories, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'érale' 'ête' 'íålittin']\n",
      "42672\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as skl_text\n",
    "\n",
    "vectorizer = skl_text.TfidfVectorizer()\n",
    "corpus_X = vectorizer.fit_transform(news_ds_train.data).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus_X, news_ds_train.target, test_size = 0.25, random_state = 0)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Multinomiale Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant entrainer un modèle appelé *Multinomial Naive Bayes* sur ce corpus. Les labels du corpus se trouvent dans la variable *news_ds_train.target*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precison du score est 96.21318373071529\n"
     ]
    }
   ],
   "source": [
    "#Commencer par importer le module Multinomiale\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Instancier le model\n",
    "model_Multinomial = MultinomialNB()\n",
    "#Entrianner le model\n",
    "model_Multinomial.fit(X_train, y_train)\n",
    "#Prediction\n",
    "prediction = model_Multinomial.predict(X_test)\n",
    "#Calculer le score \n",
    "from sklearn.metrics import accuracy_score\n",
    "precision = accuracy_score(y_test, prediction)*100\n",
    "print('La precison du score est', precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc de code suivant, nous utilisons notre modèle entrainé sur des phrases pré-sélectionnées. Est-ce que votre modèle semble performant par rapport aux thématiques choisies ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Your post is based on the premise that the laws as they stand do not discriminate anybody' => sci.med\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['Your post is based on the premise that the laws as they stand do not discriminate anybody', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# Vectorisons avec TF-IDF ces phrases\n",
    "X_new_tfidf = vectorizer.fit_transform(docs_new)\n",
    "\n",
    "predicted = model_Multinomial.predict(corpus_X)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, news_ds_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Avec le model de l'arbre de decision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precison du score est 79.38288920056101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#instanciation\n",
    "model_DT = DecisionTreeClassifier()\n",
    "#training\n",
    "model_DT.fit(X_train, y_train)\n",
    "#Prediction\n",
    "prediction = model_DT.predict(X_test)\n",
    "#Calculer le score\n",
    "from sklearn.metrics import accuracy_score\n",
    "precision = accuracy_score(y_test, prediction)*100\n",
    "print('La precison du score est', precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de faire une predictionn des phrases sur le model entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Your post is based on the premise that the laws as they stand do not discriminate anybody' => sci.med\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['Your post is based on the premise that the laws as they stand do not discriminate anybody', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# Vectorisons avec TF-IDF ces phrases\n",
    "X_new_tfidf = vectorizer.fit_transform(docs_new)\n",
    "\n",
    "predicted = model_DT.predict(corpus_X)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, news_ds_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
